# ğŸ”„ Complete Data Flow - GDGC Leaderboard

## Simple Flow (As You Wanted):

```
1. Python Script Scrapes Data (187 profiles)
   â†“
2. Data Saved to MongoDB (replaces old data)
   â†“
3. Backend Serves Data to Frontend from MongoDB
   â†“
4. On Restart/New Scrape: MongoDB data gets replaced
```

---

## Detailed Flow:

### ğŸš€ Backend Startup (index.js starts)
```javascript
1. Server starts on PORT 4000
   â†“
2. connectMongoDB() â†’ Connects to MongoDB Atlas
   â†“
3. initCache() runs:
   - Tries to load data from MongoDB
   - If MongoDB has data â†’ CACHE = MongoDB data âœ…
   - If MongoDB empty â†’ loads from local JSON (fallback)
   - CACHE now has 187 profiles in memory
   â†“
4. Server ready to serve /api/leaderboard
```

### ğŸ“Š Frontend Requests Data
```javascript
GET /api/leaderboard
   â†“
res.json(CACHE.data)  // Instant response from memory
   â†“
Frontend receives 187 profiles
```

### ğŸ• Every 20 Minutes (Cron Job)
```javascript
cron.schedule('*/20 * * * *')
   â†“
1. runScrape() starts
   - Spawns: python3 batch_from_csv.py
   â†“
2. Python scrapes all 187 profiles from Google Cloud Skills
   â†“
3. Python writes to: data/results.json
   â†“
4. Backend reads the new data
   â†“
5. saveToMongoDB(newData) â†’ REPLACES all profiles in MongoDB
   - Each profile upserted by URL (unique key)
   - Old data with same URL = updated
   - New URLs = inserted
   â†“
6. CACHE.data = newData (memory updated)
   â†“
7. Frontend fetches and sees new data immediately
```

### ğŸ”„ Backend Restarts (Deploy/Crash)
```javascript
1. Server restarts
   â†“
2. connectMongoDB() â†’ Connects to Atlas
   â†“
3. initCache() â†’ Loads 187 profiles from MongoDB
   â†“
4. CACHE ready with last scraped data âœ…
   â†“
5. No data loss! Frontend keeps working
```

---

## Current Status âœ…

### What's Working:
- âœ… MongoDB connection successful
- âœ… 187 profiles already in MongoDB
- âœ… Backend loads from MongoDB on startup
- âœ… Scraping every 20 minutes
- âœ… Auto-save to MongoDB after each scrape
- âœ… Frontend auto-fetches every 5 seconds
- âœ… Data persists across restarts

### Files:
```
GDGc-Backend/
â”œâ”€â”€ index.js                     # Main server (YOU ARE HERE)
â”œâ”€â”€ batch_from_csv.py            # Python scraper
â”œâ”€â”€ gform.csv                    # 187 profile URLs
â”œâ”€â”€ data/
â”‚   â””â”€â”€ results.json             # Latest scrape (local backup)
â””â”€â”€ results_from_gform.json      # Fallback data
```

### MongoDB:
```
Database: gdgc-leaderboard
Collection: profiles
Documents: 187 profiles
Index: url (unique)
```

---

## Simple Test Commands:

### 1. Check Current Data in MongoDB:
```bash
node test-mongo.js
```

### 2. Start Backend Locally:
```bash
cd C:/Users/hp/OneDrive/Desktop/scraper/GDGc-Backend
$env:PORT=4000
node index.js
```

### 3. Test API:
```bash
# Get all profiles
curl http://localhost:4000/api/leaderboard

# Check status
curl http://localhost:4000/api/status

# Manual scrape
curl -X POST http://localhost:4000/api/scrape
```

### 4. Test Frontend:
```bash
cd C:/Users/hp/OneDrive/Desktop/scraper/leaderboard-website
npm run dev
```
Open: http://localhost:5173

---

## How Data Gets Replaced:

### MongoDB Upsert Strategy:
```javascript
{
  updateOne: {
    filter: { url: "https://profile-url" },  // Find by URL
    update: { $set: { ...newData } },        // Replace data
    upsert: true                              // Insert if not exists
  }
}
```

### What Happens:
- **Same URL** â†’ Data updated (name, badges, titles replaced)
- **New URL** â†’ New profile inserted
- **Old URL not in new scrape** â†’ Stays in MongoDB (not deleted)

### Simple Replacement Logic:
```
Scrape completes â†’ 187 new profiles
   â†“
For each profile:
   - Find in MongoDB by URL
   - If exists: UPDATE with new data
   - If not exists: INSERT new profile
   â†“
MongoDB now has latest data
```

---

## Summary (Bilkul Simple):

1. **Scraping**: Python scrapes â†’ 187 profiles
2. **Saving**: Backend saves to MongoDB (replaces by URL)
3. **Serving**: Frontend gets data from MongoDB (via backend cache)
4. **Restart**: Backend loads from MongoDB â†’ No data loss

**Bas! Itna hi simple hai! ğŸ‰**

---

## Ready to Deploy?

Your backend is 100% ready! Just:
1. Push to GitHub (already done âœ…)
2. Add `MONGODB_URI` env var in Render
3. Backend auto-deploys
4. Frontend connects and works! ğŸš€
